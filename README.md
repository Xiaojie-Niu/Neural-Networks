# Neural-Networks
The performance of ReLU, TanH and Sigmoid for the division of three different activation functions in different layers~
## Neural network linking situation
Input layer, output layer, middle full link
Middle 2 hidden layers (Hidden Layers): four neurons in the first hidden layer, two neurons in the second hidden layer

<img width="700" alt="image" src="https://user-images.githubusercontent.com/114042177/191404917-c55b1f05-9619-44bd-85dd-d96095271d3c.png">

## Results
### ReLU activation function performance
<img width="567" alt="image" src="https://user-images.githubusercontent.com/114042177/191405108-e12945ba-f510-48af-b447-d5091ac54d0d.png">
<img width="570" alt="image" src="https://user-images.githubusercontent.com/114042177/191405144-5d6d5c9e-7ad9-4aa4-8f85-65e3cfd7ef3e.png">
<img width="574" alt="image" src="https://user-images.githubusercontent.com/114042177/191405159-f500edb1-b7c0-4835-8a9a-83fa8ce66245.png">

### TanH activation function performance
<img width="574" alt="image" src="https://user-images.githubusercontent.com/114042177/191405223-ce69c7b9-fc6d-40b2-8986-b4a25bde7ff8.png">
<img width="574" alt="image" src="https://user-images.githubusercontent.com/114042177/191405242-f95f2a44-bae2-4003-aa5e-bde4fb6566d6.png">
<img width="577" alt="image" src="https://user-images.githubusercontent.com/114042177/191405262-abce30a1-cb38-46d1-8b92-3d77cc8a618e.png">

### Sigmod activation function performance
<img width="569" alt="image" src="https://user-images.githubusercontent.com/114042177/191405309-fc283eb6-e757-4cbe-ad21-91932a8392fb.png">
<img width="577" alt="image" src="https://user-images.githubusercontent.com/114042177/191405339-76d0c6a6-a292-4cff-a7c3-b02b8141eb33.png">
<img width="579" alt="image" src="https://user-images.githubusercontent.com/114042177/191405364-e32f91fc-467a-4984-a5f4-a694b7e35126.png">

This is a class assignment from the end of 2018 year (senior year), sharing the code. I may have forgotten what it means in some places over time...
